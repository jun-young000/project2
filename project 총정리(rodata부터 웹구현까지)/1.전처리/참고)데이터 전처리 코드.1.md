```sql
#데이터 전처리 코드
price = spark.read.format("csv").option("header","true").loac("data/yong_file/공시지가.csv")
price.groupBy("지역구","기간").agg(round(avg(col("지가")),-1).alias("평균지가")).sort("지역구").show(100,False)
===============================================
police_man = spark.read.format("csv").option("header","true").load("data/yong_file/서울시_경찰관수.csv")
===============================================
police_box = spark.read.format("csv").option("header","true").load("data/yong_file/파출소현황.csv")
police_box.select(split(col("주소")," ")[1].alias("지역구")).show()
police_box.select(split(col("주소")," ")[1].alias("지역구"),"*").sort("지역구").show()
===============================================
center = spark.read.format("csv").option("header","true").load("data/yong_file/치안센터현황.csv")
center.select(split(col("주소")," ")[0].alias("지역구"),"*").sort("지역구").show()
===============================================

```

```sql
#서울시 건물대장 크롤링 데이터 전처리
struct = spark.read.format("csv").option("header","true").option("encoding","euc-kr").load("data/프젝_2_건축물대장.csv")
struct1 = struct.select(col("주용도"),col("건물위치"))
#필요한 컬럼만 새로운 변수에 저장

struct1.groupBy(col("주용도")).count()
#컬럼에 존재하는 값 확인

struct2 = struct1.na.drop()
#확인결과 null값이 존재하여 삭제

struct2.spark.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/engineer/data/seoul_building_type")
#필요한 데이터 형태를 만든 후 csv 파일로 저장
```

```sql
##ERD 생성에 용이하도록 데이터 편집


#여성안전 지킴이집
women = 
spark.read.format("csv").option("header","true").load("data/yong_file_/women_safe_zone.csv")
W = women.select(monotonically_increasing_id().alias("ID"),"*").show()
#새로운 변수에 id값을 추가한 데이터 저장

W.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/ubuntu/final_data/여성안전지킴이집_현황.csv")
#새로운 csv파일로 저장

#경찰관 현황
pm = spark.read.format("csv").option("header","true").load("data/yong_file_/police_man_count.csv")

police_man = pm.select(monotonically_increasing_id().alias("ID"),"*").withColumnRenamed("2017년","2017").withColumnRenamed("2018년","2018").withColumnRenamed("2019년","2019")
#컬럼 이름 조정이 필요하여 시행

police_man.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/ubuntu/final_dat
a/서울시_경찰관_현황.csv")

#서울시 파출소 현황
pol = spark.read.format("csv").option("header","true").load("data/yong_file_/police_center_loc.csv")

police_center = pol.select(monotonically_increasing_id().alias("ID"),"*")

police_center.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/ubuntu/final_data/서울시_파출소_현황.csv")

#서울시 치안센터 현황
c = spark.read.format("csv").option("header","true").load("data/yong_file_/police_box_loc.csv")

police_box = c.select(monotonically_increasing_id().alias("ID"),"*")

police_box.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/ubuntu/final_data/서울시_치안센터_현황.csv")

#공시지가
p = spark.read.format("csv").option("header","true").load("data/yong_file_/official_price.csv")

official_price = p.select(monotonically_increasing_id().alias("ID"),"*")

official_price.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/ubuntu/final_data/서울시_공시지가_현황.csv")

#노후건물
no = spark.read.format("csv").option("header","true").load("/user/ubuntu/data/jaeyoung_file/서울_노후건물_전처리.csv")

old = no.select(monotonically_increasing_id().alias("ID"),"*")

old.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/ubuntu/final_data/서울시_노후건물_현황.csv")

#영업 유흥업소
no = spark.read.format("csv").option("header","true").load("/user/ubuntu/data/jaeyoung_file/서울_영업중_유흥업소.csv")

old = no.select(monotonically_increasing_id().alias("ID"),"지역구","인허가일자","상세영업상태명","지번주소")
#컬럼 순서를 변경

old.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/ubuntu/final_data/서울시_유흥업소_영업_현황.csv")

#폐업 유흥업소
no = spark.read.format("csv").option("header","true").load("/user/ubuntu/data/jaeyoung_file/서울_폐업_유흥업소.csv")

old = no.sort("지역구").select(monotonically_increasing_id().alias("ID"),"지역구","인허가일자","상세영업상태명","지번주소").show()
#컬럼 순서를 변경

old.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/ubuntu/final_data/서울시_유흥업소_폐업_현황.csv")

#5대범죄
no = spark.read.format("csv").option("header","true").load("/user/ubuntu/data/sojin_file/서울시_5대범죄_연도별.csv")

old = no.select(monotonically_increasing_id().alias("ID"),"*").show()
#컬럼 순서를 변경

old.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/ubuntu/final_data/서울시_5대범죄_현황.csv")

#cctv
no = spark.read.format("csv").option("header","true").load("/user/ubuntu/data/sojin_file/서울시_cctv_연도별.csv")

old = no.select(monotonically_increasing_id().alias("ID"),"*")


old.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/ubuntu/final_data/서울시_cctv_현황.csv")

#여성1인가구
no = spark.read.format("csv").option("header","true").load("/user/ubuntu/data/sojin_file/전처리_여자1인가구_연도별.csv")

old = no.select(monotonically_increasing_id().alias("ID"),"*")


old.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/ubuntu/final_data/서울시_여성1인가구_현황.csv")

#유동인구
no = spark.read.format("csv").option("header","true").load("/user/ubuntu/data/raehyeok_file/서울_유동인구.csv")

old = no.select(monotonically_increasing_id().alias("ID"),"*")


old.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/ubuntu/final_data/서울시_유동인구_현황.csv")

#인구밀도
no = spark.read.format("csv").option("header","true").load("/user/ubuntu/data/raehyeok_file/서울_인구밀도.csv")

old = no.select(monotonically_increasing_id().alias("ID"),"*")

old.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/ubuntu/final_data/서울시_인구밀도_현황.csv")

#외국인
no = spark.read.format("csv").option("header","true").load("/user/ubuntu/data/jun_file/df_end.csv")

a = df이름.select(monotonically_increasing_id().alias("ID"),"*").withColumnRenamed("년도","기간")

old.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/ubuntu/final_data/서울시_외국인_현황.csv")
#컬럼명 변경


#성비
no1 = spark.read.format("csv").option("header","true").load("/user/ubuntu/data/jun_file/df_final.csv")

old = no1.select(monotonically_increasing_id().alias("ID"),"*").withColumnRenamed("년도","기간")
#컬럼명 변경

old.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/user/ubuntu/final_data/서울시_성비_현황.csv")
```

```sql
#기본키 설정을 위해 data 내용 수정
a = spark.read.format("csv").option("header","true").load("data/final_data/서울시_인구밀도_현황.csv")
a1 = a.withColumnRenamed("기간","기간코드").withColumnRenamed("지역구","지역코드")
a2 = a1.replace(["2017","2018","2109"],["17","18","19"],"기간코드")
a3 = a2.replace(["은평구","서대문구","마포구","양천구","강서구","구로구","금천구","영등포구","동작구","송파구","강동구","종로구","중구","용산구","성동구","광진구","동대문구","성북구","강북구","도봉구","관악구","서초구","강남구","중랑구","노원구"],["111181","111191","111201","111301","111212","111221","111281","111231","111241","111273","111274","111123","111121","111131","111142","111141","111152","111161","111291","111171","111251","111262","111261","111151","111311"],"지역코드")
#지역명과 기간을 코드로 변경
a3.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("data/real_final/서울시_인구밀도_현황.csv")

#치안센터
a = spark.read.format("csv").option("header","true").load("data/final_data/서울시_치안센터_현황.csv")
a1 = a.withColumnRenamed("지역구","지역코드")
a2 = a1.replace(["은평구","서대문구","마포구","양천구","강서구","구로구","금천구","영등포구","동작구","송파구","강동구","종로구","중구","용산구","성동구","광진구","동대문구","성북구","강북구","도봉구","관악구","서초구","강남구","중랑구","노원구"],["111181","111191","111201","111301","111212","111221","111281","111231","111241","111273","111274","111123","111121","111131","111142","111141","111152","111161","111291","111171","111251","111262","111261","111151","111311"],"지역코드")
a2.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("data/real_final/서울시_치안센터_현황.csv")

#파출소
a = spark.read.format("csv").option("header","true").load("data/final_data/서울시_파출소_현황.csv")
a1 = a.withColumnRenamed("지역구","지역코드")
a2 = a1.replace(["은평구","서대문구","마포구","양천구","강서구","구로구","금천구","영등포구","동작구","송파구","강동구","종로구","중구","용산구","성동구","광진구","동대문구","성북구","강북구","도봉구","관악구","서초구","강남구","중랑구","노원구"],["111181","111191","111201","111301","111212","111221","111281","111231","111241","111273","111274","111123","111121","111131","111142","111141","111152","111161","111291","111171","111251","111262","111261","111151","111311"],"지역코드")
a2.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("data/real_final/서울시_파출소_현황.csv")


#여성안전지킴이집
a = spark.read.format("csv").option("header","true").load("data/final_data/여성안전지킴이집_현황.csv")
a1 = a.withColumnRenamed("지역구","지역코드")
a2 = a1.replace(["은평구","서대문구","마포구","양천구","강서구","구로구","금천구","영등포구","동작구","송파구","강동구","종로구","중 구","용산구","성동구","광진구","동대문구","성북구","강북구","도봉구","관악구","서초구","강남구","중랑구","노원구"],["111181","111191","111201","111301","111212","111221","111281","111231","111241","111273","111274","111123","111121","111131","111142","111141","111152","111161","111291","111171","111251","111262","111261","111151","111311"],"지역코드")
a2.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("data/real_final/서울시_여성안전지킴이집_현황.csv")
```

```sql
#일부 컬럼의 이름을 행으로 옮기는 작업
a1 = a.select(monotonically_increasing_id().alias("ID"),"*")

se = a.select("ID",lit(2017),"지역코드",col("2017").alias("개수"))
ei = a.select("ID",lit(2018),"지역코드",col("2018").alias("개수"))
ni = a.select("ID",lit(2019),"지역코드",col("2019").alias("개수"))
#각각의 컬럼명을 행으로 집어넣고 값은 남겨서 총 3개의 df로 쪼갬
a = spark.read.format("csv").option("header","true").load("data/repair/서울시_5대범죄.csv")
a = a.replace("중 구","111121","지역코드")
a = a.replace("북구","111161","지역코드")
a = a.replace("성북로","111161","지역코드")
a = a.replace(["양천구목동중앙로23","영동대로"],["111301","111261"],"지역코드")
a = a.replace("동대문","111152","지역코드")

a = spark.read.format("csv").option("header","true").load("/user/engineer/repair/서울시_cctv.csv")
>>> a1 = a.replace("중 구","111121","지역코드")

a1.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("data/repair/서울시_cctv_현황.csv")
a = spark.read.format("csv").option("header","true").load("/user/engineer/repair/서울시_치안센터_현황.csv")

a = a.drop("지역코드")
a = a.select("ID",split(col("주소")," ")[1].alias("지역코드"),"주소","파출소명").show()
5대범죄 = a 
서울시_외국인 = c
인구밀도 = b
b = spark.read.format("csv").option("header","true").load("/user/engineer/fin/서울시_외국인.csv")

a=se.union(ei)
a=a.union(ni)



window = Window.orderBy(col('기간코드'))
a = a.withColumn('increasing_id', row_number().over(window))


a = b.select(expr("increasing_id-1").alias("ID"),col("2017").alias("기간코드"),"지역코드","개수")

se = a.select(lit("17").alias("기간코드"),"경찰서명",col("2017").alias("인원수"))
ei = a.select(lit("18").alias("기간코드"),"경찰서명",col("2018").alias("인원수"))
ni = a.select(lit("19").alias("기간코드"),"경찰서명",col("2019").alias("인원수"))



```

