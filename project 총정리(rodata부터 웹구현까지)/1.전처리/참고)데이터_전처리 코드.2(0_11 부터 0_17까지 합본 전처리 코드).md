cd C:\Users\SOJIN JO\Desktop\Project2\엔지니어링_도쿄

ssh -i "de-c4(tokyo).pem" ubuntu@52.197.177.14



<pyspark 들어가기>

hdfs dfs -put project /user/engineer/

hdfs dfs -ls data

hdfs dfs -ls data/sojin_file



**IMPORT

from pyspark.sql.functions import col

from pyspark.sql.functions import *

## 전처리 _ 1인가구 여자 연령별

single_house=spark.read.format("csv").option("header","true").option("encoding","utf8").load("data/sojin_file/서울시_1인가구_여자_연령별.csv")

single_house=spark.read.format("csv").option("header","true").option("encoding","utf8").load("project/서울시_1인가구_여자_연령별.csv")

single_house1 = single_house.drop(single_house._c0)

single_house2=single_house1.where(col("성별") == "여자")

++single_house2 = single_house1.select("*",col(구분).alias("지역구"))

++spark.sql("SELECT * FROM single_house1 WHERE (col("성별")=='여자'")

single_house3=single_house2.withColumnRenamed("구분","지역구")

++retails.select(col("InvoiceNo").alias("a"),"*").drop(retails.InvoiceNo).show()

single_house4 = single_house3.drop(single_house.성별)

single_house5=single_house4.orderBy(col("기간"), col("합계").desc())

single_house6 = single_house5.filter(col('지역구')!='합계')

single_house6.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/project/전처리_여자1인가구_연도별.csv")

single_house6=spark.read.format("csv").option("header","true").option("encoding","utf8").load("/project/전처리_여자1인가구_연도별.csv")



## 전처리_서울시_5대범죄연도별

criminal_seoul=spark.read.format("csv").option("header","true").option("encoding","utf8").load("project/서울시_5대범죄_년도별.csv")

criminal_seoul=spark.read.format("csv").option("header","true").option("encoding","utf8").load("data/sojin_file/서울시_1인가구_여자_연령별.csv")

criminal_seoul1 = criminal_seoul.drop(criminal_seoul._c0)

criminal_seoul2=criminal_seoul1 .withColumnRenamed("자치구","지역구")

> columns_to_drop = ['합계.1','살인.1','강도.1','강간강제추행.1','절도.1','폭력.1']
>
> criminal_seoul3=criminal_seoul2.drop(*columns_to_drop)
>
> criminal_seoul3.show()

criminal_seoul4 = criminal_seoul3.filter(col('기간') != '기간')

criminal_seoul4.show()

criminal_seoul5 = criminal_seoul4.filter(col('지역구')!='합계')

criminal_seoul5.show()

criminal_seoul6=criminal_seoul5.orderBy(col("기간"), col("합계").desc())

criminal_seoul6.show()

+저장

criminal_seoul6.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/project/서울시_5대범죄_연도별.csv")

criminal_seoul6=spark.read.format("csv").option("header","true").option("encoding","utf8").load("/project/sojin_data.csv")



## 전처리 5대범죄율(검거율까지 뽑는거)다시 전처리

#데이터 불러오기

criminal_seoul=spark.read.format("csv").option("header","true").option("encoding","utf8").load("project/서울시_5대범죄_년도별.csv")

#첫번째 컬럼 삭제

criminal_seoul1 = criminal_seoul.drop(criminal_seoul._c0)

#자치구를 지역구로 컬럼명 변경

criminal_seoul2=criminal_seoul1 .withColumnRenamed("자치구","지역구")

#기간으로 들어온 첫번째 row 삭제

criminal_seoul3 = criminal_seoul2.filter(col('기간') != '기간')

#컬럼명 전체적으로 변경

criminal_seoul4=criminal_seoul3 .withColumnRenamed("지역구","지역구").withColumnRenamed("합계","합계발생").withColumnRenamed("합계.1","합계검거").withColumnRenamed("살인","살인발생").withColumnRenamed("살인.1","살인검거").withColumnRenamed("강도","강도발생").withColumnRenamed("강도.1","강도검거").withColumnRenamed("강간강제추행","강간강제추행발생").withColumnRenamed("강간강제추행.1","강간강제추행검거").withColumnRenamed("절도","절도발생").withColumnRenamed("절도.1","절도검거").withColumnRenamed("폭력","폭력발생").withColumnRenamed("폭력.1","폭력검거")

#컬럼순서 재배치

criminal_seoul5 = criminal_seoul4.select ('기간','지역구','합계발생','살인발생','강도발생','강간강제추행발생','절도발생','폭력발생','합계검거','살인검거','강도검거','강간강제추행검거','절도검거','폭력검거') 

#부분적으로 컬럼명 다시 변경

> criminal_seoul6 = criminal_seoul5.withColumnRenamed("합계발생","발생합계")
>
> criminal_seoul7=criminal_seoul6.withColumnRenamed("합계검거","검거합계")

#합계가 속한 row 삭제

criminal_seoul8 = criminal_seoul7.filter(col('지역구')!='합계')

#정렬하기

criminal_seoul9=criminal_seoul8.orderBy(col("발생합계").desc()).orderBy(col("검거합계").desc())

#부분코드저장

criminal_seoul9.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/project/서울시_5대범죄_발생_검거.csv")

hdfs dfs -get /project/서울시_5대범죄_발생_검거.csv ./

#코드 설정하기(기간코드,지역코드)

criminal_seoul10 = criminal_seoul9.withColumnRenamed("기간","기간코드")

criminal_seoul11 = criminal_seoul10.replace(["2017","2018","2019"],["17","18","19"],"기간코드")

criminal_seoul12 = criminal_seoul11.withColumnRenamed("지역구","지역코드")

criminal_seoul13 = criminal_seoul12.replace(["은평구","서대문구","마포구","양천구","강서구","구로구","금천구","영등포구","동작구","송파구","강동구","종로구","중구","용산구","성동구","광진구","동대문구","성북구","강북구","도봉구","관악구","서초구","강남구","중랑구","노원구"],["111181","111191","111201","111301","111212","111221","111281","111231","111241","111273","111274","111123","111121","111131","111142","111141","111152","111161","111291","111171","111251","111262","111261","111151","111311"],"지역코드")

#아이디추가하기
criminal_seoul14 = criminal_seoul13.select(monotonically_increasing_id().alias("ID"),"*")

#아이디 추가 후 숫자가 뻥튀기될때
from pyspark.sql import Window
window=Window.orderBy(col('기간코드'))
criminal_seoul14=criminal_seoul13.withColumn('increasing_id', row_number().over(window))
criminal_seoul15= criminal_seoul14.select(expr("increasing_id-1").alias("ID"),"*")
criminal_seoul15.show()

#컬럼명삭제
criminal_seoul16=criminal_seoul15.drop("increasing_id")
criminal_seoul18 = criminal_seoul17.drop(criminal_seoul17.ID)
criminal_seoul18  = criminal_seoul17.filter(col('ID') != '0')

criminal_seoul13.withColumn('ID',row_number()).show()
criminal_seoul13.rdd.zipWithIndex().toDF().show()

criminal_seoul17 = criminal_seoul16.select ('ID','기간코드','지역코드','검거합계','살인검거','강도검거','강간강제추행검거','절도검거','폭력검거') 

#저장
criminal_seoul17.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/project/서울시_5대범죄_검거.csv")

hdfs dfs -get /project/서울시_5대범죄_검거.csv ./

## 전처리_서울시_CCTV_년도별

cctv_seoul=spark.read.format("csv").option("encoding","euc-kr").load("project/서울시_CCTV_년도별.csv")

cctv_seoul1=cctv_seoul.select('_c0','_c1','_c8','_c9','_c10')

cctv_seoul1.show()

cctv_seoul2 =cctv_seoul1.filter(col('_c0') != 'null')

cctv_seoul2 =cctv_seoul1.filter(col('_c1') != 'null')

cctv_seoul2.show()

#빈데이터프레임생성

> > from pyspark.sql.types import StructField
> >
> > from pyspark.sql.types import StructType
> >
> > from pyspark.sql.types import StringType
> >
> > schema = StructType([StructField('지역구',StringType(),True),
> >
> > StructField('합계',StringType(),True),
> >
> > StructField('2017',StringType(),True),
> >
> > StructField('2018',StringType(),True),
> >
> > StructField('2019',StringType(),True),])
> >
> > df=df_event_spark = spark.createDataFrame([],schema)
>
> cctv_seoul3=df.show(cctv_seoul2)
>
> cctv_seoul3.show()

cctv_seoul4 =cctv_seoul3.filter(col('지역구') !='계')

cctv_seoul5 =cctv_seoul4.filter(col('지역구') !='구분')

cctv_seoul5.show()

cctv_seoul6=cctv_seoul5.orderBy(col("합계").desc())

cctv_seoul6.show()

cctv_seoul6.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/project/서울시_cctv_연도별.csv")

csv_new= spark.read.format('csv').option('header','true').load('/전처리_여자1인가구_연도별.csv//part-00000-*.csv')

## 하둡에서 우분투로 가져오기 

hdfs dfs -get /project/서울시_5대범죄_연도별.csv ./

hdfs dfs -get /project/서울시_cctv_연도별.csv ./

hdfs dfs -get /project/전처리_여자1인가구_연도별.csv ./



## aws에서 파일 올리기 

hdfs dfs -ls /

hdfs dfs -ls /user

hdfs dfs -ls /user/ubuntu

hdfs dfs -ls /user/ubuntu/data 

hdfs dfs -ls /user/ubuntu/data/sojin_file

hdfs dfs -put sojin_file /user/ubuntu/data

ls

hdfs dfs -rm -r /user/ubuntu/data/sojin_file

hdfs dfs -put sojin_file /user/ubuntu/data

hdfs dfs -ls /user/ubuntu/data

hdfs dfs -ls /user/ubuntu/data/sojin_file 

hdfs dfs -put 전처리_여자1인가구_연도별.csv/user/engineer/



#파일불러오기

csv_new= spark.read.format('csv').option('header','true').load('전처리_여자1인가구_연도별.csv/part-00000-*.csv')

cctv_seoul =  spark.read.format('csv').option('header','true').load('서울시_cctv_연도별.csv/part-00000-*.csv')

## 빈데이터 만들고 리스트 만들고 row 추가하기

schema = StructType([StructField('기간코드',StringType(),True),StructField('기간',StringType(),True),])

year_code=spark.createDataFrame([],schema)

year_code.show()

year_list=[['17','2017'],['18','2018'],['19','2019']]

year_code=spark.createDataFrame(year_list)

year_code_new = year_code.union(year_code)

year_code_new.show()



## 지역코드,기간코드 바꾸기

#### 1. 서울시_cctv.csv

a = df이름.select(monotonically_increasing_id().alias("ID"),"*")

from pyspark.sql.functions import monotonically_increasing_id

cctv_seoul1 = cctv_seoul.select(monotonically_increasing_id().alias("ID"),"*")

cctv_seoul2 = cctv_seoul1.withColumnRenamed("지역구","지역코드")

a1 = a.replace(["2017","2018","2019"],["17","18","19"],"기간")

cctv_seoul3 = cctv_seoul2.replace(["은평구","서대문구","마포구","양천구","강서구","구로구","금천구","영등포구","동작구","송파구","강동구","종로구","중구","용산구","성동구","광진구","동대문구","성북구","강북구","도봉구","관악구","서초구","강남구","중랑구","노원구"],["111181","111191","111201","111301","111212","111221","111281","111231","111241","111273","111274","111123","111121","111131","111142","111141","111152","111161","111291","111171","111251","111262","111261","111151","111311"],"지역코드")

#### cctv 하둡에 저장하기

cctv_seoul_new=cctv_seoul3.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/project/서울시_cctv.csv")

#### cctv 하둡에서 우분투로 가져오기

hdfs dfs -get /project/서울시_cctv.csv ./



#### 2. 서울시_5대범죄_현황.csv

hdfs dfs -put 서울시_5대범죄_연도별.csv /user/engineer/

criminal_seoul = spark.read.format('csv').option('header','true').load('서울시_5대범죄_연도별.csv/part-00000-*.csv')

##### 이름명 바꾸기 

criminal_seoul1=criminal_seoul.withColumnRenamed("기간","기간코드").withColumnRenamed("지역구","지역코드")

##### 기간코드 바꾸기

criminal_seoul2 = criminal_seoul1.replace(["2017","2018","2019"],["17","18","19"],"기간코드")

##### 지역코드 바꾸기

criminal_seoul3 = criminal_seoul2.replace(["은평구","서대문구","마포구","양천구","강서구","구로구","금천구","영등포구","동작구","송파구","강동구","종로구","중 구","용산구","성동구","광진구","동대문구","성북구","강북구","도봉구","관악구","서초구","강남구","중랑구","노원구"],["111181","111191","111201","111301","111212","111221","111281","111231","111241","111273","111274","111123","111121","111131","111142","111141","111152","111161","111291","111171","111251","111262","111261","111151","111311"],"지역코드")



##### 저장

criminal_seoul_new=criminal_seoul3.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/project/서울시_5대범죄.csv")



##### 우분투로 가져오기

hdfs dfs -get /project/서울시_5대범죄.csv ./





#### 3. 서울시_노후건물현황

hdfs dfs -put 서울시_노후건물_현황.csv /user/engineer/

oldbuilding_seoul = spark.read.format('csv').option('header','true').load('서울시_노후건물_현황.csv/part-00000-*.csv')

oldbuilding_seoul1=oldbuilding_seoul.withColumnRenamed("기간","기간코드").withColumnRenamed("지역구","지역코드")

oldbuilding_seoul2 = oldbuilding_seoul1.replace(["2017","2018","2019"],["17","18","19"],"기간코드")

oldbuilding_seoul3 = oldbuilding_seoul2.replace(["은평구","서대문구","마포구","양천구","강서구","구로구","금천구","영등포구","동작구","송파구","강동구","종로구","중구","용산구","성동구","광진구","동대문구","성북구","강북구","도봉구","관악구","서초구","강남구","중랑구","노원구"],["111181","111191","111201","111301","111212","111221","111281","111231","111241","111273","111274","111123","111121","111131","111142","111141","111152","111161","111291","111171","111251","111262","111261","111151","111311"],"지역코드")

oldbuilding_seoul_new=oldbuilding_seoul3.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/project/서울시_노후건물.csv")

hdfs dfs -get /project/서울시_노후건물.csv ./

-----------------------------------



oldbuilding_seoul = spark.read.format('csv').option('header','true').load('서울시_노후건물.csv/part-00000-*.csv')

sb_seoul = spark.read.format('csv').option('header','true').load('서울시_성비_현황_code.csv/part-00000-*.csv')

hdfs dfs -put 서울시_성비_현황_code.csv /user/engineer

sb_seoul1=sb_seoul.select(monotonically_increasing_id().alias("ID"),"*").withColumnRenamed("년도","기간")

sb_seoul1=sb_seoul.select(monotonically_increasing_id().alias("ID"),"*")

gender_seoul_new=gender_seoul.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/project/서울시_성비.csv")

hdfs dfs -get /project/서울시_성비.csv ./

hdfs dfs -put 서울시_여성안전지킴이집_현황.csv /user/engineer/

jip_seoul = spark.read.format('csv').option('header','true').load('서울시_여성안전지킴이집_현황.csv/part-00000-*.csv')



#### 외국인현황 전처리 

hdfs dfs -put 서울시_외국인_현황.csv /user/engineer/

foreigner_seoul = spark.read.format('csv').option('header','true').load('서울시_외국인_현황.csv/part-00000-*.csv')

foreigner_seoul1 = foreigner_seoul.filter(col('지역코드')!='합계')

foreigner_seoul2 = foreigner_seoul1.filter(col('기간코드')!='2020')

foreigner_seoul_new=foreigner_seoul2.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/project/서울시_외국인.csv")

hdfs dfs -get /project/서울시_외국인.csv ./



### 서울_유흥 전처리

seoul_alcohol = spark.read.format("csv").option("header","true").option("encoding","utf8").load("서울_유흥.csv")

seoul_open3 = seoul_open2.replace(["은평구","서대문구","마포구","양천구","강서구","구로구","금천구","영등포구","동작구","송파구","강동구","종로구","중구","용산구","성동구","광진구","동대문구","성북구","강북구","도봉구","관악구","서초구","강남구","중랑구","노원구"],["111181","111191","111201","111301","111212","111221","111281","111231","111241","111273","111274","111123","111121","111131","111142","111141","111152","111161","111291","111171","111251","111262","111261","111151","111311"],"지역구")

seoul_open4=seoul_open3.select(monotonically_increasing_id().alias("ID"),"*")

seoul_open5=seoul_open4.withColumnRenamed("지역구","지역코드")

seoul_open6 = seoul_open5.select("ID","지역코드","인허가일자","지번주소")

seoul_open7 = seoul_open6.filter(seoul_open.지번주소 != '-')

##### 서울시 인허가일자 년도별로 자르기

seoul_open8 = seoul_open7.select(col('ID'),col('지역코드'), substring(seoul_open7.인허가일자, 1, 4).alias('기간'), col('인허가일자'), col('지번주소'))

##### 년도를 기간코드로 바꿔주기

 seoul_open9= seoul_open8.replace(["2017","2018","2019"],["17","18","19"],"기간")

seoul_open8.where(col("기간") <= 2017).show(5)

 seoul_open9 = spark.sql("SELECT * FROM WHERE seoul_open8 (COL("기간") <= 2017"))

seoul_open9 = seoul_open8.groupBy('기간').agg(count('인허가일자'))

seoul_open10=seoul_open9.orderBy(col("기간").desc())

seoul_open9 =seoul_open8.filter(col('기간') !='1899')

seoul_open10 =seoul_open9.filter(col('기간') !='1904')

 seoul_open11= seoul_open10.replace(['1967','1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017','2018','2019'],['17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17','17','18','19'],"기간")

 seoul_open12= seoul_open11.withColumnRenamed("기간","기간코드")

seoul_open_new=seoul_open12.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/project/서울시_유흥업소.csv")



-------------------------

#### 서울시 유동인구 현황 전처리

people_seoul_new = spark.read.format('csv').option('header','true').load('서울시_유동인구_현황.csv/part-00000-*.csv')

```python
people_seoul1 = people_seoul.select(col("총생활인구수").alias("ad_id"),
                       col("part").alias("part_id"),
                       func.round(col("new_bid"), 2).alias("bid"))
```



people_seoul_new1 = people_seoul_new.select("*",bround(col("총생활인구수"),0).alias("총생활인구수"),bround(col("일최대인구수"),0).alias("일최대인구수"),bround(col("일최소인구수"),0).alias("일최소인구수"),bround(col("주간인구수"),0).alias("주간인구수"),bround(col("야간인구수"),0).alias("야간인구수"),bround(col("일최대이동인구수"),0).alias("일최대이동인구수"),bround(col("자치구간이동인구수"),0).alias("자치구간이동인구수"))



people_seoul_new1 = people_seoul_new.select("*",bround(col("총생활인구수")).alias("총생활인구수")

-----------------

### mysql에 db저장을 위한 전처리(컬럼명지우기)

sb_seoul_new= spark.read.format('csv').load('서울시_성비.csv/part-00000-*.csv')

sb_seoul1= sb_seoul.drop(sb_seoul.ID).collect()

> sb_seoul2=sb_seoul1.where(col("_c0").like("%코드")==False)
>
> sb_seoul2.show()

sb_seoul2 =sb_seoul_new.filter(col('_c0')!='ID')

sb_seoul2.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/project/서울시_성비_fin.csv")

hdfs dfs -get /project/서울시_성비_fin.csv ./

-----------

hdfs dfs -put 서울시_노후건물.csv /user/engineer

oldb_seoul= spark.read.format('csv').load('서울시_노후건물.csv/part-00000-*.csv')

oldb_seoul1 =oldb_seoul.filter(col('_c0')!='ID')

oldb_seoul1.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/project/서울시_노후건물_fin.csv")

hdfs dfs -get /project/서울시_노후건물_fin.csv ./

-------------------

hdfs dfs -put 서울시_공시지가_현황_code.csv /user/engineer

jiga_seoul= spark.read.format('csv').option("header","true").load('서울시_공시지가_현황_fin.csv/part-00000-*.csv')

jiga_seoul1 =jiga_seoul.filter(col('_c0')!='ID')

jiga_seoul1.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/project/서울시_공시지가_fin.csv")

hdfs dfs -get /project/서울시_공시지가_fin.csv ./

-----------------

cctv_total1= spark.read.format('csv').load('서울시_cctv현황.csv/part-00000-*.csv')

hdfs dfs -put 서울시_cctv현황.csv /user/engineer/

cctv_total2 =cctv_total1.filter(col('_c0')!='ID')

cctv_total2.coalesce(1).write.format("csv").mode("overwrite").save("/project/서울시_cctv_현황_fin.csv")

hdfs dfs -get /project/서울시_cctv_현황_fin.csv ./



hdfs dfs -put 서울시_cctv_현황_fin.csv /user/engineer/

cctv_total= spark.read.format('csv').load('서울시_cctv_현황_fin.csv/part-00000-*.csv')

from pyspark.sql.functions import *

cctv_total1=cctv_total.select(monotonically_increasing_id().alias("ID"),"*")

cctv_total2 =cctv_total1.filter(col('_c0')!='기간코드')

cctv_total2.coalesce(1).write.format("csv").mode("overwrite").save("/project/서울시_cctv_현황_최종.csv")

hdfs dfs -get /project/서울시_cctv_현황_최종.csv ./



## monotonically_increasing_id() 오류 뜰때 (순서가 뻥튀기될때)

window = Window.orderBy(col('기간코드'))
a = a.withColumn('increasing_id', row_number().over(window))
a = b.select(expr("increasing_id-1").alias("ID"),col("2017").alias("기간코드"),"지역코드","개수")
